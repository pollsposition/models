{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook follows `munic_model_prod.ipynb`, where we ran the model. Here, we'll extract and format the out-of-sample data, to then push them through the model and get predictions for Paris 2020 city-council elections. Then, we'll analyze and plot the results in the notebook `munic_model_analysis.ipynb`. You don't have to understand the model to read this notebook, but if you're curious about it, please go ahead and read the other notebook!\n",
    "\n",
    "So, as usual, let's start by importing the necessary packages and defining handy helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "\n",
    "import arviz as az\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import pymc3 as pm\n",
    "\n",
    "from bokeh.palettes import brewer\n",
    "from scipy.special import softmax\n",
    "\n",
    "SPAN_POLLS = 5\n",
    "ALPHA_POLLS = 2 / (SPAN_POLLS + 1)\n",
    "CANDIDATES = {\n",
    "    \"Simonnet\": \"farleft\",\n",
    "    \"Hidalgo\": \"left\",\n",
    "    \"Belliard\": \"green\",\n",
    "    \"Buzyn\": \"center\",\n",
    "    \"Griveaux\": \"center\",\n",
    "    \"Dati\": \"right\",\n",
    "    \"Federbusch\": \"farright\",\n",
    "}\n",
    "MONTHS = {\"janvier\": 1, \"février\": 2, \"mars\": 3}\n",
    "PARTIES = [\"farleft\", \"left\", \"green\", \"center\", \"right\", \"farright\", \"other\"]\n",
    "Nparties = len(PARTIES) - 1\n",
    "PARTIES_AGG = [\n",
    "    \"farleft_agg\",\n",
    "    \"left_agg\",\n",
    "    \"green_agg\",\n",
    "    \"center_agg\",\n",
    "    \"right_agg\",\n",
    "    \"farright_agg\",\n",
    "]\n",
    "RIGHT_POLLSTER = {\n",
    "    \"Harris Interactive\": \"Harris\",\n",
    "    \"Ifop-Fiducial\": \"Ifop\",\n",
    "    \"Ipsos-Sopra Steria\": \"Ipsos\",\n",
    "}\n",
    "BINS = np.array([15.0, 25.0, 35.0, 45.0, 55.0, 65.0, 75.0])\n",
    "COLORS = {\n",
    "    \"farleft\": np.array(brewer[\"Reds\"][7][::-1]),\n",
    "    \"left\": np.array(brewer[\"PuRd\"][7][::-1]),\n",
    "    \"green\": np.array(brewer[\"Greens\"][7][::-1]),\n",
    "    \"center\": np.array(brewer[\"Oranges\"][7][::-1]),\n",
    "    \"right\": np.array(brewer[\"Blues\"][7][::-1]),\n",
    "    \"farright\": np.array(brewer[\"Purples\"][7][::-1]),\n",
    "    \"other\": np.array(brewer[\"Greys\"][7][::-1]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.style.use(\"arviz-darkgrid\")\n",
    "\n",
    "\n",
    "def compute_analyt_weights(df: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    pollster_ratings = pd.read_csv(\"../data/polls_1st_round/pollsters_weights.csv\")\n",
    "    df = pd.merge(\n",
    "        df, pollster_ratings, how=\"left\", left_on=\"pollster\", right_on=\"sondage\"\n",
    "    )\n",
    "\n",
    "    for p in PARTIES[:-1]:\n",
    "        df[f\"weightsondeur_{p}\"].fillna(\n",
    "            pollster_ratings[f\"weightsondeur_{p}\"].median(), inplace=True\n",
    "        )\n",
    "        df[f\"analyt_weights_{p}\"] = np.log(df.samplesize) * df[f\"weightsondeur_{p}\"]\n",
    "\n",
    "    return df.set_index(\"date\").sort_index()\n",
    "\n",
    "\n",
    "def agg_polls(df: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    unique_dates = sorted(set(df.index))\n",
    "\n",
    "    for p in PARTIES[:-1]:\n",
    "        for i, d_outer in enumerate(unique_dates):\n",
    "            for j, d_inner in enumerate(unique_dates[: i + 1]):\n",
    "                df.loc[d_inner, f\"expon_weights_{p}\"] = (1 - ALPHA_POLLS) ** (i - j)\n",
    "\n",
    "            df[f\"final_weights_{p}\"] = (\n",
    "                df[f\"analyt_weights_{p}\"] * df[f\"expon_weights_{p}\"]\n",
    "            )\n",
    "            final_weights = df.loc[:d_outer, f\"final_weights_{p}\"]\n",
    "            vote_share = df.loc[:d_outer, f\"{p}\"]\n",
    "\n",
    "            df.loc[d_outer, f\"{p}_agg\"] = np.average(vote_share, weights=final_weights)\n",
    "\n",
    "            # compute aggregate sample size only once:\n",
    "            if p == \"right\":\n",
    "                # same weights, whatever the party:\n",
    "                expon_weights = df.loc[:d_outer, \"expon_weights_right\"]\n",
    "                sample_size = df.loc[:d_outer, \"samplesize\"]\n",
    "                df.loc[d_outer, \"samplesize_agg\"] = round(\n",
    "                    np.average(sample_size, weights=expon_weights)\n",
    "                )\n",
    "\n",
    "    return df.reset_index()[\n",
    "        [\"date\", \"samplesize_agg\"] + [f\"{p}_agg\" for p in PARTIES[:-1]]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's load our MCMC samples and data -- nothing really thrilling here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alex_andorra/opt/anaconda3/envs/fundfix/lib/python3.7/site-packages/ipykernel_launcher.py:14: FutureWarning: Creating a PeriodIndex by passing range endpoints is deprecated.  Use `pandas.period_range` instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "with open(\"trace_prod.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "m_prod, trace_prod = data[\"model\"], data[\"trace\"]\n",
    "idata = az.from_pymc3(trace_prod)\n",
    "post = idata.posterior\n",
    "\n",
    "unemp = pd.read_excel(\n",
    "    \"../data/predictors/chomage-zone-demploi-2003-2019.xls\",\n",
    "    header=5,\n",
    "    sheet_name=\"txcho_ze\",\n",
    ")\n",
    "unemp = unemp[unemp[\"LIBZE2010\"] == \"Paris\"].iloc[:, 4:].T\n",
    "unemp.columns = [\"unemployment\"]\n",
    "unemp.index = pd.PeriodIndex(start=unemp.index[0], periods=len(unemp), freq=\"Q\")\n",
    "\n",
    "d = pd.read_csv(\"../data/whole_formatted_data.csv\", index_col=0)\n",
    "district_id, districts = d.arrondissement.factorize(sort=True)\n",
    "Ndistricts = len(districts)\n",
    "type_id, types = d.type.factorize(sort=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is something more fun: we're going to scrape the most recent polls from the appropriate [Wikipedia page](https://fr.wikipedia.org/wiki/%C3%89lections_municipales_de_2020_%C3%A0_Paris). Indeed, our model is trained on polls and unemployment data from previous elections. To get predictions for the coming city-council elections (March 15th, 2020), we need the last [unemployment figures in Paris](https://www.insee.fr/fr/statistiques/1893230) (most recent are for Q3 2019) and the last polls. Here is how the scraping of polls goes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pollster</th>\n",
       "      <th>date</th>\n",
       "      <th>samplesize</th>\n",
       "      <th>farleft</th>\n",
       "      <th>left</th>\n",
       "      <th>green</th>\n",
       "      <th>center</th>\n",
       "      <th>right</th>\n",
       "      <th>farright</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ifop</td>\n",
       "      <td>2020-01-17</td>\n",
       "      <td>955</td>\n",
       "      <td>5.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ifop</td>\n",
       "      <td>2020-01-17</td>\n",
       "      <td>955</td>\n",
       "      <td>5.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Odoxa</td>\n",
       "      <td>2020-01-20</td>\n",
       "      <td>879</td>\n",
       "      <td>8.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Odoxa</td>\n",
       "      <td>2020-01-23</td>\n",
       "      <td>916</td>\n",
       "      <td>4.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>14.5</td>\n",
       "      <td>16.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ipsos</td>\n",
       "      <td>2020-02-19</td>\n",
       "      <td>1000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Harris</td>\n",
       "      <td>2020-02-19</td>\n",
       "      <td>1092</td>\n",
       "      <td>6.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Odoxa</td>\n",
       "      <td>2020-02-19</td>\n",
       "      <td>809</td>\n",
       "      <td>7.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Ifop</td>\n",
       "      <td>2020-02-21</td>\n",
       "      <td>976</td>\n",
       "      <td>6.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Elabe</td>\n",
       "      <td>2020-02-28</td>\n",
       "      <td>1001</td>\n",
       "      <td>5.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>9.5</td>\n",
       "      <td>18.5</td>\n",
       "      <td>25.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Ifop</td>\n",
       "      <td>2020-02-28</td>\n",
       "      <td>946</td>\n",
       "      <td>5.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Harris</td>\n",
       "      <td>2020-03-02</td>\n",
       "      <td>1119</td>\n",
       "      <td>5.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pollster       date  samplesize  farleft  left  green  center  right  \\\n",
       "0      Ifop 2020-01-17         955      5.0  25.0   14.0    15.0   19.0   \n",
       "1      Ifop 2020-01-17         955      5.0  25.0   14.0    16.0   17.0   \n",
       "2     Odoxa 2020-01-20         879      8.0  24.0   13.0    16.0   18.0   \n",
       "3     Odoxa 2020-01-23         916      4.0  23.0   14.5    16.0   20.0   \n",
       "4     Ipsos 2020-02-19        1000      5.0  24.0   13.0    19.0   20.0   \n",
       "5    Harris 2020-02-19        1092      6.0  23.0   13.0    17.0   23.0   \n",
       "6     Odoxa 2020-02-19         809      7.0  23.0   14.0    17.0   25.0   \n",
       "7      Ifop 2020-02-21         976      6.0  24.0   12.0    19.0   22.0   \n",
       "8     Elabe 2020-02-28        1001      5.0  24.0    9.5    18.5   25.0   \n",
       "9      Ifop 2020-02-28         946      5.0  24.0   11.0    20.0   25.0   \n",
       "10   Harris 2020-03-02        1119      5.0  24.0   11.0    17.0   25.0   \n",
       "\n",
       "    farright  \n",
       "0        5.0  \n",
       "1        5.0  \n",
       "2        5.0  \n",
       "3        6.0  \n",
       "4        4.0  \n",
       "5        5.0  \n",
       "6        4.0  \n",
       "7        3.5  \n",
       "8        4.0  \n",
       "9        3.5  \n",
       "10       4.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_polls = pd.read_html(\n",
    "    \"https://fr.wikipedia.org/wiki/%C3%89lections_municipales_de_2020_%C3%A0_Paris\",\n",
    "    attrs={\"class\": \"wikitable centre\"},\n",
    "    match=\"Date de réalisation\",\n",
    "    decimal=\",\",\n",
    "    thousands=\" \",\n",
    "    na_values=[\"—\", \"?\"],\n",
    ")[0]\n",
    "raw_polls.columns = raw_polls.columns.droplevel([0, 2])\n",
    "raw_polls = raw_polls[\n",
    "    ~raw_polls.Source.str.contains(\"candidature | annonce | renonce\")\n",
    "].drop([\"Gantzer\", \"Villani\", \"Bournazel\", \"Campion\", \"Berkani\", \"Autres\"], axis=1)\n",
    "\n",
    "# clean polls' characteristics:\n",
    "raw_polls = raw_polls.rename(\n",
    "    columns={\n",
    "        \"Source\": \"pollster\",\n",
    "        \"Date de réalisation\": \"date\",\n",
    "        \"Échantillon\": \"samplesize\",\n",
    "    }\n",
    ")\n",
    "raw_polls[\"pollster\"] = raw_polls.pollster.replace(RIGHT_POLLSTER)\n",
    "raw_polls[\"samplesize\"] = raw_polls[\"samplesize\"].str.split().str.join(\"\").astype(int)\n",
    "\n",
    "# take last field date:\n",
    "field = raw_polls[\"date\"].str.split(\" au \", expand=True)[1].str.split(expand=True)\n",
    "field.columns = [\"day\", \"month\"]\n",
    "field[\"month\"] = field[\"month\"].replace(MONTHS)\n",
    "field[\"year\"] = 2020\n",
    "raw_polls[\"date\"] = pd.to_datetime(field[[\"day\", \"month\", \"year\"]])\n",
    "\n",
    "# clean candidates' values:\n",
    "raw_polls[list(CANDIDATES.keys())] = raw_polls[CANDIDATES.keys()].astype(float)\n",
    "raw_polls[\"Buzyn\"] = raw_polls[[\"Buzyn\", \"Griveaux\"]].fillna(0).sum(axis=1)\n",
    "raw_polls = (\n",
    "    raw_polls.drop(\"Griveaux\", axis=1)\n",
    "    .rename(columns=CANDIDATES)\n",
    "    .sort_values(\"date\")\n",
    "    .dropna()\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "raw_polls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty nice, uh? Yeah, pandas is awesome! Now, we're going to aggregate those polls by recency, sample size and historical performance of the pollster. This last weight is based on [a pollster ratings I computed](https://www.pollsposition.com/indicateurs/pollster_ratings) and updated with last year's polls (2019 European elections) -- I didn't have time to open-source this analysis yet, but hopefully one day I will! I'm not going to detail everything by writing, but you can see how it's done in the code of the two functions we defined at the beginning -- `compute_analyt_weights` and `agg_polls`. The code below also transforms the polls from their natural habitat ($[0, 1]$) to the real line ($[-\\infty, +\\infty]$. This is for technical reasons that I'm not going to detail here -- if you're curious, I explain everything in the notebook of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>samplesize_agg</th>\n",
       "      <th>farleft_agg</th>\n",
       "      <th>left_agg</th>\n",
       "      <th>green_agg</th>\n",
       "      <th>center_agg</th>\n",
       "      <th>right_agg</th>\n",
       "      <th>farright_agg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-17</td>\n",
       "      <td>955.0</td>\n",
       "      <td>-2.00</td>\n",
       "      <td>-0.39</td>\n",
       "      <td>-0.97</td>\n",
       "      <td>-0.86</td>\n",
       "      <td>-0.71</td>\n",
       "      <td>-2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-20</td>\n",
       "      <td>922.0</td>\n",
       "      <td>-1.83</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>-0.99</td>\n",
       "      <td>-0.85</td>\n",
       "      <td>-0.71</td>\n",
       "      <td>-2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-23</td>\n",
       "      <td>920.0</td>\n",
       "      <td>-1.94</td>\n",
       "      <td>-0.43</td>\n",
       "      <td>-0.97</td>\n",
       "      <td>-0.84</td>\n",
       "      <td>-0.67</td>\n",
       "      <td>-1.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-02-19</td>\n",
       "      <td>950.0</td>\n",
       "      <td>-1.87</td>\n",
       "      <td>-0.45</td>\n",
       "      <td>-1.01</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>-0.55</td>\n",
       "      <td>-2.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2020-02-21</td>\n",
       "      <td>956.0</td>\n",
       "      <td>-1.85</td>\n",
       "      <td>-0.44</td>\n",
       "      <td>-1.03</td>\n",
       "      <td>-0.73</td>\n",
       "      <td>-0.54</td>\n",
       "      <td>-2.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2020-02-28</td>\n",
       "      <td>963.0</td>\n",
       "      <td>-1.91</td>\n",
       "      <td>-0.44</td>\n",
       "      <td>-1.14</td>\n",
       "      <td>-0.70</td>\n",
       "      <td>-0.47</td>\n",
       "      <td>-2.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2020-03-02</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>-1.94</td>\n",
       "      <td>-0.43</td>\n",
       "      <td>-1.16</td>\n",
       "      <td>-0.71</td>\n",
       "      <td>-0.45</td>\n",
       "      <td>-2.19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date  samplesize_agg  farleft_agg  left_agg  green_agg  center_agg  \\\n",
       "0  2020-01-17           955.0        -2.00     -0.39      -0.97       -0.86   \n",
       "2  2020-01-20           922.0        -1.83     -0.40      -0.99       -0.85   \n",
       "3  2020-01-23           920.0        -1.94     -0.43      -0.97       -0.84   \n",
       "4  2020-02-19           950.0        -1.87     -0.45      -1.01       -0.75   \n",
       "7  2020-02-21           956.0        -1.85     -0.44      -1.03       -0.73   \n",
       "8  2020-02-28           963.0        -1.91     -0.44      -1.14       -0.70   \n",
       "10 2020-03-02          1001.0        -1.94     -0.43      -1.16       -0.71   \n",
       "\n",
       "    right_agg  farright_agg  \n",
       "0       -0.71         -2.00  \n",
       "2       -0.71         -2.00  \n",
       "3       -0.67         -1.92  \n",
       "4       -0.55         -2.07  \n",
       "7       -0.54         -2.12  \n",
       "8       -0.47         -2.18  \n",
       "10      -0.45         -2.19  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oos_polls = compute_analyt_weights(raw_polls)\n",
    "oos_polls = agg_polls(oos_polls).drop_duplicates()\n",
    "# revert the softmax:\n",
    "oos_polls[PARTIES_AGG] = oos_polls[PARTIES_AGG].div(100).apply(np.log) + 1\n",
    "oos_polls.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, we're ready to make out-of-sample predictions! This is going to look weird and esoteric, but I'm actually just taking each parameters for the couples (district, party) and pushing them through the model to get the posterior probabilities of each party, in each district -- this will make more sense when we get to the vizualization part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 8000, 6)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_unemployment = (\n",
    "    (np.log(unemp.iloc[-1]) - np.log(d[\"unemployment\"]).mean())\n",
    "    / np.log(d[\"unemployment\"]).std()\n",
    ").iloc[0]\n",
    "\n",
    "# extract posterior std of noise:\n",
    "noisy_mus_sd = az.summary(trace_prod, round_to=2, var_names=[\"noisy_mus\"])[\"sd\"]\n",
    "post_sd_noisy_mus = []\n",
    "for p in range(Nparties):\n",
    "    post_sd_noisy_mus.append(\n",
    "        noisy_mus_sd.loc[[f\"noisy_mus[{idx},{p}]\" for idx in range(trace_prod[\"noisy_mus\"].shape[1])]].mean()\n",
    "    )\n",
    "post_sd_noisy_mus = np.asarray(post_sd_noisy_mus)\n",
    "\n",
    "# push parameters through the model:\n",
    "post_preds = []\n",
    "for p_id, p in enumerate(PARTIES_AGG):\n",
    "    last_polls = (oos_polls.iloc[-1][p] - d[p].mean()) / d[p].std()\n",
    "\n",
    "    post_preds.append(\n",
    "        trace_prod[f\"β_district_p{p_id}\"][:, :, 0]\n",
    "        + trace_prod[f\"type_effect_p{p_id}\"][:, 2, None]\n",
    "        + trace_prod[f\"β_district_p{p_id}\"][:, :, 1] * last_unemployment\n",
    "        + trace_prod[f\"β_district_p{p_id}\"][:, :, 2] * last_polls\n",
    "    )\n",
    "    \n",
    "post_preds = np.asarray(post_preds).T\n",
    "noisy_post_preds = post_preds + pm.Normal.dist(\n",
    "    mu=[0.0] * Nparties, sigma=post_sd_noisy_mus\n",
    ").random(size=post_preds.shape[0:2])\n",
    "noisy_post_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 8000, 7)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vary_pivot = pm.Normal.dist(mu=-3.0, sigma=0.05).random(\n",
    "    size=(Ndistricts, post_preds.shape[1], 1)\n",
    ")\n",
    "noisy_post_preds = np.c_[noisy_post_preds, vary_pivot]\n",
    "noisy_post_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 8000, 7)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "share_est = softmax(noisy_post_preds, axis=2) * 100\n",
    "share_mean = share_est.mean(1)\n",
    "np.savez_compressed(\"oos_data/post_share_est\", share_est=share_est)\n",
    "share_est.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the proportions for each party, in each district! But we'll also want to map those results on the geography of Paris. To do that, we'll compute how is the most likely winner in each district, what's its probability of winning, and which proportion of the votes it's expected to get. This is done by the code below. I want to sincerely thank Grégoire David for [his awesome open-source project, france-geojson](https://github.com/gregoiredavid/france-geojson), where I found the geographic shapes of Paris."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "win_summary = {}\n",
    "for district in range(Ndistricts):\n",
    "    district_shares = share_est[district]\n",
    "    district_means = share_mean[district]\n",
    "    district_hpds = az.hpd(district_shares)\n",
    "\n",
    "    # compute winner in each simulation:\n",
    "    order = np.squeeze(np.argsort(-district_shares, axis=1)[:, :1])\n",
    "    winners = pd.Series(order)\n",
    "\n",
    "    probs = pd.DataFrame(\n",
    "        winners.value_counts(normalize=True).multiply(100).round().astype(int)\n",
    "    )\n",
    "    probs.columns = [\"odds\"]\n",
    "    probs[\"low\"], probs[\"high\"], probs[\"mean\"] = np.nan, np.nan, np.nan\n",
    "\n",
    "    for p_id in probs.index:\n",
    "        probs.loc[p_id, \"low\"], probs.loc[p_id, \"high\"] = district_hpds[p_id]\n",
    "        probs.loc[p_id, \"mean\"] = district_means[p_id]\n",
    "        # assign means to color bins:\n",
    "        bins_idx = np.digitize(probs.loc[p_id, \"mean\"], BINS)\n",
    "        probs.loc[p_id, \"color\"] = COLORS[PARTIES[p_id]][bins_idx]\n",
    "    probs.index = np.array(PARTIES)[probs.index]\n",
    "    probs.index.name = \"winner\"\n",
    "    win_summary[district] = probs.reset_index()\n",
    "\n",
    "win_summary = pd.concat(win_summary).reset_index(level=1)\n",
    "# keep only most probable winner:\n",
    "win_summary = (\n",
    "    win_summary[win_summary.level_1 == 0].drop(\"level_1\", axis=1).set_index(districts)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>code</th>\n",
       "      <th>nom</th>\n",
       "      <th>geometry</th>\n",
       "      <th>winner</th>\n",
       "      <th>odds</th>\n",
       "      <th>low</th>\n",
       "      <th>high</th>\n",
       "      <th>mean</th>\n",
       "      <th>color</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>75101</td>\n",
       "      <td>Paris 1er Arrondissement</td>\n",
       "      <td>POLYGON ((2.32576 48.86955, 2.32787 48.86986, ...</td>\n",
       "      <td>Left</td>\n",
       "      <td>91</td>\n",
       "      <td>25.100554</td>\n",
       "      <td>32.960564</td>\n",
       "      <td>29.105330</td>\n",
       "      <td>#c994c7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>75102</td>\n",
       "      <td>Paris 2e  Arrondissement</td>\n",
       "      <td>POLYGON ((2.35084 48.86334, 2.32787 48.86986, ...</td>\n",
       "      <td>Left</td>\n",
       "      <td>80</td>\n",
       "      <td>25.526296</td>\n",
       "      <td>34.048202</td>\n",
       "      <td>29.966632</td>\n",
       "      <td>#c994c7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>75103</td>\n",
       "      <td>Paris 3e  Arrondissement</td>\n",
       "      <td>POLYGON ((2.35009 48.86195, 2.35084 48.86334, ...</td>\n",
       "      <td>Left</td>\n",
       "      <td>99</td>\n",
       "      <td>32.180484</td>\n",
       "      <td>41.409152</td>\n",
       "      <td>36.739671</td>\n",
       "      <td>#df65b0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>75104</td>\n",
       "      <td>Paris 4e  Arrondissement</td>\n",
       "      <td>POLYGON ((2.34456 48.85399, 2.35009 48.86195, ...</td>\n",
       "      <td>Left</td>\n",
       "      <td>98</td>\n",
       "      <td>29.305109</td>\n",
       "      <td>37.735143</td>\n",
       "      <td>33.596215</td>\n",
       "      <td>#c994c7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>75105</td>\n",
       "      <td>Paris 5e  Arrondissement</td>\n",
       "      <td>POLYGON ((2.34456 48.85399, 2.36432 48.84617, ...</td>\n",
       "      <td>Left</td>\n",
       "      <td>94</td>\n",
       "      <td>29.552118</td>\n",
       "      <td>38.201596</td>\n",
       "      <td>34.056986</td>\n",
       "      <td>#c994c7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index   code                       nom  \\\n",
       "0      1  75101  Paris 1er Arrondissement   \n",
       "1      2  75102  Paris 2e  Arrondissement   \n",
       "2      3  75103  Paris 3e  Arrondissement   \n",
       "3      4  75104  Paris 4e  Arrondissement   \n",
       "4      5  75105  Paris 5e  Arrondissement   \n",
       "\n",
       "                                            geometry winner  odds        low  \\\n",
       "0  POLYGON ((2.32576 48.86955, 2.32787 48.86986, ...   Left    91  25.100554   \n",
       "1  POLYGON ((2.35084 48.86334, 2.32787 48.86986, ...   Left    80  25.526296   \n",
       "2  POLYGON ((2.35009 48.86195, 2.35084 48.86334, ...   Left    99  32.180484   \n",
       "3  POLYGON ((2.34456 48.85399, 2.35009 48.86195, ...   Left    98  29.305109   \n",
       "4  POLYGON ((2.34456 48.85399, 2.36432 48.84617, ...   Left    94  29.552118   \n",
       "\n",
       "        high       mean    color  \n",
       "0  32.960564  29.105330  #c994c7  \n",
       "1  34.048202  29.966632  #c994c7  \n",
       "2  41.409152  36.739671  #df65b0  \n",
       "3  37.735143  33.596215  #c994c7  \n",
       "4  38.201596  34.056986  #c994c7  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paris_shape = (\n",
    "    gpd.read_file(\"../data/paris_shape.json\")\n",
    "    .sort_values(\"code\")\n",
    "    .reset_index(drop=True)\n",
    "    .set_index(districts)\n",
    ")\n",
    "paris_shape = pd.concat([paris_shape, win_summary], axis=1).reset_index()\n",
    "paris_shape[\"winner\"] = paris_shape[\"winner\"].str.title()\n",
    "\n",
    "try: \n",
    "    os.remove(\"oos_data/paris_shape.geojson\")\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "paris_shape.to_file(\"oos_data/paris_shape.geojson\", driver='GeoJSON')\n",
    "paris_shape.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've now stored all the data we need -- it's time to plot them and see what the model is telling us about the coming elections... See you in the notebook `munic_model_analysis.ipynb` or, even better, [here]()!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy     1.17.3\n",
      "arviz     0.7.0\n",
      "pandas    0.25.3\n",
      "pymc3     3.8\n",
      "geopandas 0.6.1\n",
      "AlexAndorra \n",
      "last updated: Thu Mar 05 2020 \n",
      "\n",
      "CPython 3.7.0\n",
      "IPython 7.12.0\n"
     ]
    }
   ],
   "source": [
    "%watermark -a AlexAndorra -n -u -v -iv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fundfix",
   "language": "python",
   "name": "fundfix"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
